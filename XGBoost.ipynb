{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:45.556645Z",
     "iopub.status.busy": "2022-04-18T02:30:45.556340Z",
     "iopub.status.idle": "2022-04-18T02:30:48.944994Z",
     "shell.execute_reply": "2022-04-18T02:30:48.944253Z",
     "shell.execute_reply.started": "2022-04-18T02:30:45.556563Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, f1_score\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:48.948669Z",
     "iopub.status.busy": "2022-04-18T02:30:48.948459Z",
     "iopub.status.idle": "2022-04-18T02:30:48.954388Z",
     "shell.execute_reply": "2022-04-18T02:30:48.953643Z",
     "shell.execute_reply.started": "2022-04-18T02:30:48.948644Z"
    }
   },
   "outputs": [],
   "source": [
    "# References:\n",
    "# https://www.kaggle.com/code/ashishpatel26/kfold-lightgbm\n",
    "# https://www.kaggle.com/code/sz8416/simple-bayesian-optimization-for-lightgbm/notebook\n",
    "# https://www.kaggle.com/code/jsaguiar/lightgbm-7th-place-solution\n",
    "# https://medium.com/thecyphy/home-credit-default-risk-part-2-84b58c1ab9d5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:48.956556Z",
     "iopub.status.busy": "2022-04-18T02:30:48.956009Z",
     "iopub.status.idle": "2022-04-18T02:30:48.964383Z",
     "shell.execute_reply": "2022-04-18T02:30:48.963498Z",
     "shell.execute_reply.started": "2022-04-18T02:30:48.956519Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(df):\n",
    "    \n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    \n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    cat_ohe = ohe.fit_transform(df[categorical_columns]).toarray()\n",
    "    \n",
    "    # concat the numerical columns and tranformed catrgorical columns\n",
    "    df_new = df.drop(categorical_columns, axis=1)\n",
    "    df_new2 = pd.DataFrame(cat_ohe, columns=ohe.get_feature_names_out())\n",
    "    df_all = pd.concat([df_new, df_new2], axis=1)\n",
    "    new_columns = [c for c in df_all.columns if c not in original_columns]\n",
    "    \n",
    "    return df_all, new_columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:48.966794Z",
     "iopub.status.busy": "2022-04-18T02:30:48.966516Z",
     "iopub.status.idle": "2022-04-18T02:30:48.977562Z",
     "shell.execute_reply": "2022-04-18T02:30:48.976919Z",
     "shell.execute_reply.started": "2022-04-18T02:30:48.966761Z"
    }
   },
   "outputs": [],
   "source": [
    "def application_preprocessing(train, test):\n",
    "    \n",
    "    train['CREDIT_INCOME_PERCENT'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']\n",
    "    train['ANNUITY_INCOME_PERCENT'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL']\n",
    "    train['CREDIT_TERM'] = train['AMT_ANNUITY'] / train['AMT_CREDIT']\n",
    "    train['DAYS_EMPLOYED_PERCENT'] = train['DAYS_EMPLOYED'] / train['DAYS_BIRTH']\n",
    "\n",
    "    test['CREDIT_INCOME_PERCENT'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']\n",
    "    test['ANNUITY_INCOME_PERCENT'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\n",
    "    test['CREDIT_TERM'] = test['AMT_ANNUITY'] / test['AMT_CREDIT']\n",
    "    test['DAYS_EMPLOYED_PERCENT'] = test['DAYS_EMPLOYED'] / test['DAYS_BIRTH']\n",
    "    \n",
    "    \n",
    "\n",
    "#     poly_features = train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "#     poly_features_test = test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "\n",
    "#     # imputer for handling missing values\n",
    "#     imputer = SimpleImputer(strategy = 'median')\n",
    "#     poly_features = imputer.fit_transform(poly_features)\n",
    "#     poly_features_test = imputer.transform(poly_features_test)\n",
    "    \n",
    "#     # create Polynomial Features\n",
    "#     poly_transformer = PolynomialFeatures(degree = 3)\n",
    "#     poly_features = poly_transformer.fit_transform(poly_features)\n",
    "#     poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "\n",
    "#     # Create a dataframe of the features \n",
    "#     poly_features = pd.DataFrame(poly_features, columns = poly_transformer.get_feature_names_out(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "#     poly_features_test = pd.DataFrame(poly_features_test, columns = poly_transformer.get_feature_names_out(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "#     # Merge polynomial features into training/testing dataframe\n",
    "#     poly_features['SK_ID_CURR'] = train['SK_ID_CURR']\n",
    "#     train_poly = train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "#     poly_features_test['SK_ID_CURR'] = test['SK_ID_CURR']\n",
    "#     test_poly = test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "#     # Print out the new shapes\n",
    "#     print('Training data with polynomial features shape: ', train_poly.shape)\n",
    "#     print('Testing data with polynomial features shape:  ', test_poly.shape)\n",
    "\n",
    "#     #X_train = train_poly.drop('TARGET', axis=1)\n",
    "#     #y_train = train_poly['TARGET']\n",
    "#     #X_test = test_poly.copy()\n",
    "\n",
    "    categorical_columns = [col for col in train.columns if train[col].dtype == 'object']\n",
    "    \n",
    "    class_dict = {}\n",
    "    for i in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        train[i] = le.fit_transform(train[i])\n",
    "        class_dict[i] = list(le.classes_)\n",
    "        test[i] = le.transform(test[i])\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "#    return train_poly, test_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:48.979303Z",
     "iopub.status.busy": "2022-04-18T02:30:48.978934Z",
     "iopub.status.idle": "2022-04-18T02:30:48.995365Z",
     "shell.execute_reply": "2022-04-18T02:30:48.994465Z",
     "shell.execute_reply.started": "2022-04-18T02:30:48.979168Z"
    }
   },
   "outputs": [],
   "source": [
    "def previous_application_preprocessing(df):\n",
    "    \n",
    "    df, cat_cols = one_hot_encoder(df)\n",
    "    df['DAYS_FIRST_DRAWING'].max()\n",
    "    df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    df['APP_CREDIT_PERC'] = df['AMT_CREDIT']/df['AMT_APPLICATION'] \n",
    "\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': [ 'max', 'mean'],\n",
    "        'AMT_APPLICATION': [ 'max','mean'],\n",
    "        'AMT_CREDIT': [ 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': [ 'max', 'mean'],\n",
    "        'AMT_DOWN_PAYMENT': [ 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': [ 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': [ 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': [ 'max', 'mean'],\n",
    "        'DAYS_DECISION': [ 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "        \n",
    "    df_agg = df.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    df_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "    \n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = df[df['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    df_agg = df_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = df[df['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    df_agg = df_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:48.998239Z",
     "iopub.status.busy": "2022-04-18T02:30:48.996592Z",
     "iopub.status.idle": "2022-04-18T02:30:49.014318Z",
     "shell.execute_reply": "2022-04-18T02:30:49.013626Z",
     "shell.execute_reply.started": "2022-04-18T02:30:48.998186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance_preprocessing(bureau_balance, bureau):\n",
    "\n",
    "    bb, bb_cat = one_hot_encoder(bureau_balance)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': [ 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': [ 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': [ 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': [ 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    \n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:49.017256Z",
     "iopub.status.busy": "2022-04-18T02:30:49.016713Z",
     "iopub.status.idle": "2022-04-18T02:30:49.025719Z",
     "shell.execute_reply": "2022-04-18T02:30:49.024929Z",
     "shell.execute_reply.started": "2022-04-18T02:30:49.017221Z"
    }
   },
   "outputs": [],
   "source": [
    "def credit_card_balance_preprocessing(cc):\n",
    "    \n",
    "    cc, cat_cols = one_hot_encoder(cc)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg([ 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:49.027809Z",
     "iopub.status.busy": "2022-04-18T02:30:49.026924Z",
     "iopub.status.idle": "2022-04-18T02:30:49.035802Z",
     "shell.execute_reply": "2022-04-18T02:30:49.035096Z",
     "shell.execute_reply.started": "2022-04-18T02:30:49.027775Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_cash_preprocessing(pos):\n",
    "   \n",
    "    pos, cat_cols = one_hot_encoder(pos)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:49.037421Z",
     "iopub.status.busy": "2022-04-18T02:30:49.037103Z",
     "iopub.status.idle": "2022-04-18T02:30:49.049486Z",
     "shell.execute_reply": "2022-04-18T02:30:49.048730Z",
     "shell.execute_reply.started": "2022-04-18T02:30:49.037386Z"
    }
   },
   "outputs": [],
   "source": [
    "def installments_payments_preprocessing(ins):\n",
    "    \n",
    "    ins, cat_cols = one_hot_encoder(ins)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum','min','std' ],\n",
    "        'DBD': ['max', 'mean', 'sum','min','std'],\n",
    "        'PAYMENT_PERC': [ 'max','mean',  'var','min','std'],\n",
    "        'PAYMENT_DIFF': [ 'max','mean', 'var','min','std'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum','min','std'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum','std'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum','std']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Reducing Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:49.052340Z",
     "iopub.status.busy": "2022-04-18T02:30:49.052114Z",
     "iopub.status.idle": "2022-04-18T02:30:49.065438Z",
     "shell.execute_reply": "2022-04-18T02:30:49.064635Z",
     "shell.execute_reply.started": "2022-04-18T02:30:49.052310Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:30:49.067435Z",
     "iopub.status.busy": "2022-04-18T02:30:49.067037Z",
     "iopub.status.idle": "2022-04-18T02:32:08.017895Z",
     "shell.execute_reply": "2022-04-18T02:32:08.017207Z",
     "shell.execute_reply.started": "2022-04-18T02:30:49.067396Z"
    }
   },
   "outputs": [],
   "source": [
    "# import data files\n",
    "train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\n",
    "train = reduce_mem_usage(train)\n",
    "print('train: ', train.shape)\n",
    "\n",
    "test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\n",
    "test = reduce_mem_usage(test)\n",
    "print('test: ', test.shape)\n",
    "\n",
    "previous_application = pd.read_csv('../input/home-credit-default-risk/previous_application.csv')\n",
    "previous_application = reduce_mem_usage(previous_application)\n",
    "print('previous_application: ', previous_application.shape)\n",
    "\n",
    "bureau_balance = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv')\n",
    "bureau_balance = reduce_mem_usage(bureau_balance)\n",
    "print('bureau_balance: ', bureau_balance.shape)\n",
    "\n",
    "bureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv')\n",
    "bureau = reduce_mem_usage(bureau)\n",
    "print('bureau: ', bureau.shape)\n",
    "\n",
    "credit_card_balance = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\n",
    "credit_card_balance = reduce_mem_usage(credit_card_balance)\n",
    "print('credit_card_balance: ', credit_card_balance.shape)\n",
    "\n",
    "POS_CASH_balance = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\n",
    "POS_CASH_balance = reduce_mem_usage(POS_CASH_balance)\n",
    "print('POS_CASH_balance: ', POS_CASH_balance.shape)\n",
    "\n",
    "installments_payments = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv')\n",
    "installments_payments = reduce_mem_usage(installments_payments)\n",
    "print('installments_payments: ', installments_payments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing using Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:32:08.019689Z",
     "iopub.status.busy": "2022-04-18T02:32:08.019032Z",
     "iopub.status.idle": "2022-04-18T02:33:38.324249Z",
     "shell.execute_reply": "2022-04-18T02:33:38.323395Z",
     "shell.execute_reply.started": "2022-04-18T02:32:08.019650Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing for every dataset\n",
    "train, test = application_preprocessing(train, test)\n",
    "previous_application = previous_application_preprocessing(previous_application)\n",
    "bureau_and_balance = bureau_and_balance_preprocessing(bureau_balance, bureau)\n",
    "credit_card_balance = credit_card_balance_preprocessing(credit_card_balance)\n",
    "pos_cash = pos_cash_preprocessing(POS_CASH_balance)\n",
    "installments_payments = installments_payments_preprocessing(installments_payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:33:38.326611Z",
     "iopub.status.busy": "2022-04-18T02:33:38.326323Z",
     "iopub.status.idle": "2022-04-18T02:33:40.976965Z",
     "shell.execute_reply": "2022-04-18T02:33:40.976209Z",
     "shell.execute_reply.started": "2022-04-18T02:33:38.326574Z"
    }
   },
   "outputs": [],
   "source": [
    "## merge dataset using ID\n",
    "df_train = train.join(bureau_and_balance, how='left', on='SK_ID_CURR')\n",
    "df_train = df_train.join(previous_application, how='left', on='SK_ID_CURR')\n",
    "df_train = df_train.join(pos_cash, how='left', on='SK_ID_CURR')\n",
    "df_train = df_train.join(installments_payments, how='left', on='SK_ID_CURR')\n",
    "df_train = df_train.join(credit_card_balance, how='left', on='SK_ID_CURR')\n",
    "\n",
    "df_test = test.join(bureau_and_balance, how='left', on='SK_ID_CURR')\n",
    "df_test = df_test.join(previous_application, how='left', on='SK_ID_CURR')\n",
    "df_test = df_test.join(pos_cash, how='left', on='SK_ID_CURR')\n",
    "df_test = df_test.join(installments_payments, how='left', on='SK_ID_CURR')\n",
    "df_test = df_test.join(credit_card_balance, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:33:40.978496Z",
     "iopub.status.busy": "2022-04-18T02:33:40.978238Z",
     "iopub.status.idle": "2022-04-18T02:33:47.152419Z",
     "shell.execute_reply": "2022-04-18T02:33:47.151675Z",
     "shell.execute_reply.started": "2022-04-18T02:33:40.978452Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:33:47.154045Z",
     "iopub.status.busy": "2022-04-18T02:33:47.153766Z",
     "iopub.status.idle": "2022-04-18T02:33:47.365790Z",
     "shell.execute_reply": "2022-04-18T02:33:47.364932Z",
     "shell.execute_reply.started": "2022-04-18T02:33:47.154011Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del train\n",
    "del test\n",
    "del previous_application\n",
    "del bureau_balance\n",
    "del credit_card_balance\n",
    "del POS_CASH_balance\n",
    "del installments_payments\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:33:47.367839Z",
     "iopub.status.busy": "2022-04-18T02:33:47.367571Z",
     "iopub.status.idle": "2022-04-18T02:33:47.605586Z",
     "shell.execute_reply": "2022-04-18T02:33:47.604662Z",
     "shell.execute_reply.started": "2022-04-18T02:33:47.367802Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop('TARGET', axis=1)\n",
    "y_train = df_train['TARGET']\n",
    "\n",
    "X_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:33:47.610663Z",
     "iopub.status.busy": "2022-04-18T02:33:47.609924Z",
     "iopub.status.idle": "2022-04-18T02:33:47.741471Z",
     "shell.execute_reply": "2022-04-18T02:33:47.740675Z",
     "shell.execute_reply.started": "2022-04-18T02:33:47.610621Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "X_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "X_test = X_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:33:47.743029Z",
     "iopub.status.busy": "2022-04-18T02:33:47.742625Z",
     "iopub.status.idle": "2022-04-18T02:33:47.751840Z",
     "shell.execute_reply": "2022-04-18T02:33:47.751104Z",
     "shell.execute_reply.started": "2022-04-18T02:33:47.742992Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:33:47.755080Z",
     "iopub.status.busy": "2022-04-18T02:33:47.754319Z",
     "iopub.status.idle": "2022-04-18T02:33:47.770931Z",
     "shell.execute_reply": "2022-04-18T02:33:47.769518Z",
     "shell.execute_reply.started": "2022-04-18T02:33:47.755034Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.TARGET.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:33:47.772862Z",
     "iopub.status.busy": "2022-04-18T02:33:47.772432Z",
     "iopub.status.idle": "2022-04-18T02:33:47.779636Z",
     "shell.execute_reply": "2022-04-18T02:33:47.778911Z",
     "shell.execute_reply.started": "2022-04-18T02:33:47.772791Z"
    }
   },
   "outputs": [],
   "source": [
    "print(282686/24825)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T02:36:01.669152Z",
     "iopub.status.busy": "2022-04-18T02:36:01.668848Z",
     "iopub.status.idle": "2022-04-18T02:36:01.752337Z",
     "shell.execute_reply": "2022-04-18T02:36:01.751663Z",
     "shell.execute_reply.started": "2022-04-18T02:36:01.669122Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning using Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T05:33:16.505831Z",
     "iopub.status.busy": "2022-04-18T05:33:16.505516Z",
     "iopub.status.idle": "2022-04-18T05:35:29.807515Z",
     "shell.execute_reply": "2022-04-18T05:35:29.804865Z",
     "shell.execute_reply.started": "2022-04-18T05:33:16.505794Z"
    }
   },
   "outputs": [],
   "source": [
    "def xgb_evaluation(max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel,\n",
    "                   colsample_bynode, reg_alpha, reg_lambda, scale_pos_weight):\n",
    "    '''\n",
    "    Objective function for Bayesian Optimization of XGBoost's Hyperparamters. Takes the hyperparameters as input, and\n",
    "    returns the Cross-Validation AUC as output.\n",
    "    \n",
    "    Inputs: Hyperparamters to be tuned.\n",
    "        max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel,\n",
    "        colsample_bynode, reg_alpha, reg_lambda\n",
    "        \n",
    "    Returns:\n",
    "        CV ROC-AUC Score\n",
    "    '''\n",
    "    params = {\n",
    "        'learning_rate' : 0.01,\n",
    "        'n_estimators' : 10000,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'gpu_id' : 0,\n",
    "        'max_depth' : int(round(max_depth)),\n",
    "        'min_child_weight' : int(round(min_child_weight)),\n",
    "        'subsample': subsample,\n",
    "        'gamma' : gamma,\n",
    "        'colsample_bytree' : colsample_bytree,\n",
    "        'colsample_bylevel' : colsample_bylevel,\n",
    "        'colsample_bynode' : colsample_bynode,\n",
    "        'reg_alpha' : reg_alpha,\n",
    "        'reg_lambda' : reg_lambda,\n",
    "        'scale_pos_weight' : int(round(scale_pos_weight)),\n",
    "        'random_state' : 51412\n",
    "    }    \n",
    "    \n",
    "    #defining the Cross-Validation Strategry\n",
    "    stratified_cv = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 33)\n",
    "    cv_preds = np.zeros(X_train.shape[0])\n",
    "    \n",
    "    #iterating over each fold, training the model, and making Out of Fold Predictions\n",
    "    for train_indices, cv_indices in stratified_cv.split(X_train, y_train):\n",
    "        \n",
    "        x_tr = X_train.iloc[train_indices]\n",
    "        y_tr = y_train.iloc[train_indices]\n",
    "        x_cv = X_train.iloc[cv_indices]\n",
    "        y_cv = y_train.iloc[cv_indices]\n",
    "        \n",
    "        xgbc = XGBClassifier(**params)\n",
    "        xgbc.fit(x_tr, y_tr, eval_set= [(x_cv,y_cv)],\n",
    "                        eval_metric='auc', verbose = False, early_stopping_rounds=200)\n",
    "        \n",
    "        cv_preds[cv_indices] = xgbc.predict_proba(x_cv, ntree_limit = xgbc.get_booster().best_ntree_limit)[:,1]\n",
    "        gc.collect()   \n",
    "        \n",
    "    return roc_auc_score(y_train, cv_preds)\n",
    "  \n",
    "#using the above objective function to find the optimal hyperparams\n",
    "#defining the optimizer and the hyperparameters along with ranges of values.\n",
    "bopt_xgb =  BayesianOptimization(xgb_evaluation, {'max_depth' : (5,15),\n",
    "                                                  'min_child_weight' : (5,80),\n",
    "                                                  'gamma' : (0.2,1),\n",
    "                                                  'subsample' : (0.5,1),\n",
    "                                                  'colsample_bytree' : (0.5,1),\n",
    "                                                  'colsample_bylevel' : (0.3,1),\n",
    "                                                  'colsample_bynode' : (0.3,1),\n",
    "                                                  'reg_alpha' : (0.001, 0.3),\n",
    "                                                  'reg_lambda' : (0.001, 0.3),\n",
    "                                                  'scale_pos_weight' : (1,11)}, \n",
    "                                 random_state = 55)\n",
    "\n",
    "bopt_xgb.maximize(n_iter = 10, init_points = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T04:15:06.386325Z",
     "iopub.status.busy": "2022-04-18T04:15:06.386066Z",
     "iopub.status.idle": "2022-04-18T04:15:06.393675Z",
     "shell.execute_reply": "2022-04-18T04:15:06.392867Z",
     "shell.execute_reply.started": "2022-04-18T04:15:06.386297Z"
    }
   },
   "outputs": [],
   "source": [
    "print('AUC: ', bopt_xgb.max['target'])\n",
    "print('parameters: ', bopt_xgb.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T04:15:06.796957Z",
     "iopub.status.busy": "2022-04-18T04:15:06.796466Z",
     "iopub.status.idle": "2022-04-18T04:15:06.801425Z",
     "shell.execute_reply": "2022-04-18T04:15:06.800585Z",
     "shell.execute_reply.started": "2022-04-18T04:15:06.796907Z"
    }
   },
   "outputs": [],
   "source": [
    "max_params = bopt_xgb.max['params']\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "max_params['min_child_weight'] = int(round(max_params['min_child_weight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T05:00:54.123239Z",
     "iopub.status.busy": "2022-04-18T05:00:54.122967Z",
     "iopub.status.idle": "2022-04-18T05:00:54.130410Z",
     "shell.execute_reply": "2022-04-18T05:00:54.128084Z",
     "shell.execute_reply.started": "2022-04-18T05:00:54.123211Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_xgb = xgboost.XGBClassifier(\n",
    " n_estimators = 10000,\n",
    " learning_rate = 0.01,\n",
    " colsample_bylevel = 0.916,\n",
    " colsample_bynode = 0.5289,\n",
    " colsample_bytree = 0.52050,\n",
    " gamma = 0.85416,\n",
    " max_depth = 6,\n",
    " min_child_weight = 67,\n",
    " reg_alpha = 0.24312,\n",
    " reg_lambda = 0.104283,\n",
    " subsample = 0.854,\n",
    " tree_method = 'gpu_hist',\n",
    " gpu_id = 0,\n",
    " eval_metric='auc',\n",
    " early_stopping_rounds=200,\n",
    " random_state = 51412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T05:00:55.388765Z",
     "iopub.status.busy": "2022-04-18T05:00:55.388515Z",
     "iopub.status.idle": "2022-04-18T05:06:35.720396Z",
     "shell.execute_reply": "2022-04-18T05:06:35.719687Z",
     "shell.execute_reply.started": "2022-04-18T05:00:55.388737Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T05:09:03.510440Z",
     "iopub.status.busy": "2022-04-18T05:09:03.509989Z",
     "iopub.status.idle": "2022-04-18T05:11:01.469444Z",
     "shell.execute_reply": "2022-04-18T05:11:01.468866Z",
     "shell.execute_reply.started": "2022-04-18T05:09:03.510403Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_predict = clf_xgb.predict(X_train)\n",
    "y_test_predict =clf_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T05:19:52.936788Z",
     "iopub.status.busy": "2022-04-18T05:19:52.936543Z",
     "iopub.status.idle": "2022-04-18T05:20:08.097442Z",
     "shell.execute_reply": "2022-04-18T05:20:08.096857Z",
     "shell.execute_reply.started": "2022-04-18T05:19:52.936761Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred=clf_xgb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T05:36:01.760102Z",
     "iopub.status.busy": "2022-04-18T05:36:01.759765Z",
     "iopub.status.idle": "2022-04-18T05:36:02.022443Z",
     "shell.execute_reply": "2022-04-18T05:36:02.021791Z",
     "shell.execute_reply.started": "2022-04-18T05:36:01.760062Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/home-credit-default-risk/sample_submission.csv')\n",
    "submission['TARGET'] = y_test_pred\n",
    "submission['TARGET'] = submission['TARGET'].apply(lambda x: 0 if x <0 else x)\n",
    "submission.to_csv('submission_xgb.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
